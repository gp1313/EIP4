[0.030041888836491854, 0.9903]
## Convolution
- Convolution is a mathematical operation that generates new tensor using two input tensor. 
- Each value in output tensor is generated by core operation which is element wise tensor multiplication between two tensor.
- the whole catch here is that the two tensors need not be of same size / dimension. We take the smallest one and overlap it with some part of bigger one. 
- each overlap will have core operation of element wise multiplication involved.
- The overlap starts at on end of bigger tensor and shifts in direction of not explored part of tensor iteratively.
 
## Filters / Kernels
- Filters  or kernels are just matrix with fixed values. But when you use these matrix to perform convolution of some image, the output has some interesting relationship with input.
- A typical example is edge detection using a particular 3x3 kernel or a circular blog detection or smoothing of image or sharpening of image using kernels.
- In CNN we don’t have fixed values, in fact the whole point of learning a tasking using can is to find out right values of these kernels which we initially had no clue about. I.e we started with a random guess.
  
## Epochs
- We generally don’t train neural network in one shot. i.e we don't update weight values after computing loss value for all training data points. We take a small batch of data points and feed forward using them and then update weights.
- One epoch is when we have feed the network with all data points in training set at least once.
 
## 1x1 Convolution
- It is special case of typical convolution where were the overlap is just with single value, but the interesting thing about this is that values across channel are summed up to single value at output.
- Hence effectively 1x1 convolution is equivalent summing up the input tensor in channel direction keeping tensor dimension in other direction constant.

## 3x3 Convolution
- 3x3 kernel size used in convolution makes it possible to compute most basic feature an image can have. A simple edge. We can construct many complex structures using combination of small edges. Hence 3x3 convolution acts as building block of feature extraction process in neural network.

## Feature Maps
- Output tensor after convolution can be studied as feature map. each value in feature map has some linear or non-linear mapping to bigger patch of input image.
## Activation Function
- Activation function bring in the non-linearity factor in function approximation of neural network.
- Without them any deep neural network is equivalent to a single matrix multiplication having linear mapping of output to input tensor.

## Receptive Field
- Receptive field of any value in feature map is patch of input image that it is mapped to.
